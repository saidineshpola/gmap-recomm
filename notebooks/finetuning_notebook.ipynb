{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19afbda-01d4-4208-a2d9-4597c5c1b61b",
   "metadata": {},
   "source": [
    "# Step1 getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cbd667-f65e-42d7-85b0-a293edc8dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================ ROCm System Management Interface ============================\n",
      "====================================== Product Info ======================================\n",
      "GPU[0]\t\t: Card Series: \t\tInstinct MI210\n",
      "GPU[0]\t\t: Card Model: \t\t0x740f\n",
      "GPU[0]\t\t: Card Vendor: \t\tAdvanced Micro Devices, Inc. [AMD/ATI]\n",
      "GPU[0]\t\t: Card SKU: \t\tD67301V\n",
      "GPU[0]\t\t: Subsystem ID: \t0x0c34\n",
      "GPU[0]\t\t: Device Rev: \t\t0x02\n",
      "GPU[0]\t\t: Node ID: \t\t4\n",
      "GPU[0]\t\t: GUID: \t\t53592\n",
      "GPU[0]\t\t: GFX Version: \t\tgfx9010\n",
      "==========================================================================================\n",
      "================================== End of ROCm SMI Log ===================================\n"
     ]
    }
   ],
   "source": [
    "!rocm-smi --showproductname\n",
    "# sudo apt install libdrm-dev libsystemd-dev nvtop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555cfdef-04a6-4ebe-b7bd-24b4cf01830d",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6f30fb-5d5e-4533-b3bf-992222f41ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from peft) (2.0.1+git5b5a729)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/jinja2-3.1.4-py3.9.egg (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: pytorch-triton-rocm<2.1,>=2.0.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/pytorch_triton_rocm-2.0.2-py3.9-linux-x86_64.egg (from torch>=1.13.0->peft) (2.0.2)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting eval-type-backport>=0.1.3 (from tyro>=0.5.11->trl)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch>=1.13.0->peft) (3.29.3)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch>=1.13.0->peft) (18.1.6)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.43.1-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.1-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.2/417.2 kB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, shtab, safetensors, regex, pygments, pyarrow-hotfix, numpy, mdurl, fsspec, eval-type-backport, docstring-parser, dill, pyarrow, pandas, multiprocess, markdown-it-py, huggingface-hub, tokenizers, rich, tyro, transformers, datasets, accelerate, trl, peft\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.12.0\n",
      "    Uninstalling Pygments-2.12.0:\n",
      "      Successfully uninstalled Pygments-2.12.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.26.4 which is incompatible.\n",
      "scipy 1.6.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.33.0 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 eval-type-backport-0.2.0 fsspec-2024.5.0 huggingface-hub-0.24.1 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 numpy-1.26.4 pandas-2.2.2 peft-0.11.1 pyarrow-17.0.0 pyarrow-hotfix-0.6 pygments-2.18.0 pytz-2024.1 regex-2024.5.15 rich-13.7.1 safetensors-0.4.3 shtab-1.7.1 tokenizers-0.19.1 transformers-4.43.1 trl-0.9.6 tyro-0.8.5 tzdata-2024.1 xxhash-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac33e40-9de6-40ff-9bdb-00b73bbccaaa",
   "metadata": {},
   "source": [
    "## Install bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5948a4e1-7c3e-48f0-9b57-d5ce35b98765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'bitsandbytes' already exists and is not an empty directory.\n",
      "/home/aac/bitsandbytes\n",
      "fatal: detected dubious ownership in repository at '/home/aac/bitsandbytes'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /home/aac/bitsandbytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools>=63 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from -r requirements-dev.txt (line 2)) (71.1.0)\n",
      "Requirement already satisfied: pytest~=8.2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from -r requirements-dev.txt (line 3)) (8.2.2)\n",
      "Collecting einops~=0.8.0 (from -r requirements-dev.txt (line 4))\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: wheel~=0.43.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from -r requirements-dev.txt (line 5)) (0.43.0)\n",
      "Collecting lion-pytorch~=0.1.4 (from -r requirements-dev.txt (line 6))\n",
      "  Downloading lion_pytorch-0.1.4-py3-none-any.whl.metadata (618 bytes)\n",
      "Collecting scipy~=1.13.0 (from -r requirements-dev.txt (line 7))\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas~=2.2.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from -r requirements-dev.txt (line 8)) (2.2.2)\n",
      "Collecting matplotlib~=3.8.4 (from -r requirements-dev.txt (line 9))\n",
      "  Downloading matplotlib-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytest~=8.2.0->-r requirements-dev.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytest~=8.2.0->-r requirements-dev.txt (line 3)) (24.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytest~=8.2.0->-r requirements-dev.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytest~=8.2.0->-r requirements-dev.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytest~=8.2.0->-r requirements-dev.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (2.0.1+git5b5a729)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from scipy~=1.13.0->-r requirements-dev.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2024.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9)) (10.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib~=3.8.4->-r requirements-dev.txt (line 9))\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib~=3.8.4->-r requirements-dev.txt (line 9)) (3.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.2->-r requirements-dev.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/jinja2-3.1.4-py3.9.egg (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: pytorch-triton-rocm<2.1,>=2.0.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/pytorch_triton_rocm-2.0.2-py3.9-linux-x86_64.egg (from torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (2.0.2)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (3.29.3)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (18.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from jinja2->torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from sympy->torch>=1.6->lion-pytorch~=0.1.4->-r requirements-dev.txt (line 6)) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lion_pytorch-0.1.4-py3-none-any.whl (4.3 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.0/305.0 kB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, pyparsing, kiwisolver, importlib-resources, fonttools, einops, cycler, contourpy, matplotlib, lion-pytorch\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.3\n",
      "    Uninstalling scipy-1.6.3:\n",
      "      Successfully uninstalled scipy-1.6.3\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 einops-0.8.0 fonttools-4.53.1 importlib-resources-6.4.0 kiwisolver-1.4.5 lion-pytorch-0.1.4 matplotlib-3.8.4 pyparsing-3.1.2 scipy-1.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m-- Configuring bitsandbytes (Backend: hip)\n",
      "-- NO_CUBLASLT := OFF\n",
      "-- HIP Compiler: /opt/rocm/llvm/bin/clang++\n",
      "-- HIP Targets: gfx900;gfx906;gfx908;gfx90a;gfx1030;gfx1100;gfx1101;gfx940;gfx941;gfx942\n",
      "\u001b[0mhipblas VERSION: 2.1.0\u001b[0m\n",
      "\u001b[0mhiprand VERSION: 2.10.16\u001b[0m\n",
      "\u001b[0mhipsparse VERSION: 3.0.1\u001b[0m\n",
      "-- Configuring done (0.4s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/aac/bitsandbytes\n",
      "[100%] Built target bitsandbytes\n",
      "Processing /home/aac/bitsandbytes\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from bitsandbytes==0.43.2.dev0) (2.0.1+git5b5a729)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from bitsandbytes==0.43.2.dev0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->bitsandbytes==0.43.2.dev0) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->bitsandbytes==0.43.2.dev0) (4.12.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->bitsandbytes==0.43.2.dev0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from torch->bitsandbytes==0.43.2.dev0) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/jinja2-3.1.4-py3.9.egg (from torch->bitsandbytes==0.43.2.dev0) (3.1.4)\n",
      "Requirement already satisfied: pytorch-triton-rocm<2.1,>=2.0.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages/pytorch_triton_rocm-2.0.2-py3.9-linux-x86_64.egg (from torch->bitsandbytes==0.43.2.dev0) (2.0.2)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch->bitsandbytes==0.43.2.dev0) (3.29.3)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch->bitsandbytes==0.43.2.dev0) (18.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from jinja2->torch->bitsandbytes==0.43.2.dev0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.9/lib/python3.9/site-packages (from sympy->torch->bitsandbytes==0.43.2.dev0) (1.3.0)\n",
      "Building wheels for collected packages: bitsandbytes\n",
      "  Building wheel for bitsandbytes (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bitsandbytes: filename=bitsandbytes-0.43.2.dev0-cp39-cp39-linux_x86_64.whl size=2711418 sha256=812b19999c4a64804a764f8711b8d6c1ea2163880442d81b21706c9cfe0765a4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jj9e2w7x/wheels/56/a7/0c/3a738b785a0c8c21d509c24749af3e0529f1ed58a77a5fa50a\n",
      "Successfully built bitsandbytes\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m/home/aac\n"
     ]
    }
   ],
   "source": [
    "!git clone --recurse https://github.com/ROCm/bitsandbytes\n",
    "!git config --global --add safe.directory /home/aac\n",
    "%cd /home/aac/bitsandbytes\n",
    "!git checkout rocm_enabled\n",
    "!pip install -r requirements-dev.txt\n",
    "!cmake -DCOMPUTE_BACKEND=hip -S . #Use -DBNB_ROCM_ARCH=\"gfx90a;gfx942\" to target specific gpu arch\n",
    "!make\n",
    "!pip install .\n",
    "%cd /home/aac/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7a7eac-36c0-4a37-9047-e964a7ed0c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install datasets transformers peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78258a-3043-43a4-baa6-7b8b256c026c",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bc6012-7f26-4fa8-993c-2ac94bd48b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e39cc-0c64-4b87-b231-b499f8ba1cbc",
   "metadata": {},
   "source": [
    "# Step 2: Configuring the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87662be1-1745-412e-bade-c7314a551de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [04:36<00:00, 39.48s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN']='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'\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"Meta-Llama-3-8B-Instruct\" # \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model_name = \"llama-3-8b-gmap-recomm\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct', trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    \n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "# base_model.save_pretrained('Meta-Llama-3-8B-Instruct')\n",
    "# llama_tokenizer.save_pretrained('Meta-Llama-3-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e549edaa-90c9-420e-8db9-d6c68022a986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.02k/1.02k [00:00<00:00, 14.3MB/s]\n",
      "Downloading data: 100%|██████████| 967k/967k [00:00<00:00, 2.57MB/s]\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 139572.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data set\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aaf5542-eda1-42d8-af04-38b4cd82617e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20565/20565 [00:00<00:00, 44917.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Combine relevant fields into a single text field\n",
    "    texts = [\n",
    "        f\"Question: {q}\\nContext: {c}\\nCOTAnswer: {a}\"\n",
    "        for q, c, a in zip(examples['question'], examples['oracle_context'], examples['cot_answer'])\n",
    "    ]\n",
    "    \n",
    "    # Return a dictionary with the new 'text' field\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def prepare_dataset_for_sft(dataset_path):\n",
    "    # Load the dataset\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    preprocessed_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names  # Remove original columns\n",
    "    )\n",
    "    \n",
    "    # Set the format to PyTorch tensors\n",
    "    preprocessed_dataset.set_format(type=\"torch\")\n",
    "    return preprocessed_dataset\n",
    "\n",
    "training_data = prepare_dataset_for_sft(\n",
    "    dataset_path=\"datasets/arrow_dataset\",\n",
    ")\n",
    "#training_data.save_to_disk('datasets/dataset_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b9d446-7793-4a0b-89e8-ec9054c57702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = load_from_disk('datasets/dataset_preprocessed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85846b2a-ca19-4577-afa0-20607df56f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Question: How would you rate the overall quality of service provided by Majestic Security based on user reviews?\\nContext: {\"name\": \"Majestic Security\", \"address\": \"Majestic Security, 3128 Lexington Park Dr, Elkhart, IN 46514\", \"gmap_id\": \"0x8816c4b2fb8fb6a1:0x80451636e10ca83f\", \"description\": null, \"latitude\": 41.6899261, \"longitude\": -86.02416989999999, \"category\": [\"Security guard service\", \"Business to business service\", \"Public safety office\", \"Security service\", \"Training centre\", \"Training school\", \"Transportation escort service\"], \"avg_rating\": 4.3, \"num_of_reviews\": 48, \"price\": null, \"hours\": [[\"Thursday\", \"9AM\\\\u20135PM\"], [\"Friday\", \"9AM\\\\u20135PM\"], [\"Saturday\", \"Closed\"], [\"Sunday\", \"Closed\"], [\"Monday\", \"9AM\\\\u20135PM\"], [\"Tuesday\", \"9AM\\\\u20135PM\"], [\"Wednesday\", \"9AM\\\\u20135PM\"]], \"MISC\": null, \"state\": \"Open \\\\u22c5 Closes 5PM\", \"relative_results\": [\"0x8816e8092cc37eff:0xa138075153591bc7\", \"0x8816ce61cc404e23:0x71a5e9e0898036a4\", \"0x8816e9eb8afbc539:0x7d7ee677df3fafa3\", \"0x8816cd46eed45c35:0x7d80db2d3b489fc3\", \"0x8816ebe5c65cf3ad:0xe8dede77091f4ecf\"], \"url\": \"https://www.google.com/maps/place//data=!4m2!3m1!1s0x8816c4b2fb8fb6a1:0x80451636e10ca83f?authuser=-1&hl=en&gl=us\"}\\nCOTAnswer: To rate the overall quality of service provided by Majestic Security based on user reviews, I can analyze the average rating (4.3) and the number of reviews (48). This suggests that a significant portion of customers have had positive experiences with the company.\\n\\n##begin_quote##The high average rating indicates a strong level of satisfaction among users.##end_quote##\\n\\nTaking this into account, I would rate the overall quality of service provided by Majestic Security as:\\n\\n<ANSWER>: Excellent'}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e60c79-ef64-472d-803e-f60c84678087",
   "metadata": {},
   "source": [
    "# Step 3: Start fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af771954-905a-43af-9ea4-960a58915663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified_v2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #optim=\"paged_adamw_32bit\",\n",
    "    save_steps=3000,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4520f-04e3-4c2e-8c13-f3fd8d89dfea",
   "metadata": {},
   "source": [
    "Training with LoRA configuration\n",
    "Now you can integrate LoRA into the base model and assess its additional parameters. LoRA essentially adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains the newly added weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd52f9ff-6727-4657-9c3e-547d60c1e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    target_modules= ['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4ad53-3ee7-4005-a158-e4e6f7722289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/20565 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 20565/20565 [00:04<00:00, 4763.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8245' max='41130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8245/41130 1:26:58 < 5:46:58, 1.58 it/s, Epoch 0.40/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbbe50-518f-45b2-b8e8-41fc7074181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dc7d3-16e6-4787-b014-bb5427259860",
   "metadata": {},
   "source": [
    "# Step 4: Test the fine-tuned model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d970036-8fce-4b01-b0e8-88a06f27fbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore the delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [04:37<00:00, 39.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "try:\n",
    "    del base_model\n",
    "except:\n",
    "    print('Ignore the delete')\n",
    "base_model_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "   # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "new_model_name = 'results_modified_v2/checkpoint-12000'\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a30b19f-2893-4e90-b23a-9278039a42c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is best restaurent in Indiana state US? [/INST] <s>What is best restaurant in Indiana state US?</s>\n",
      "\n",
      "The best restaurant in Indiana state US is subjective and depends on personal preferences. However, according to online reviews and ratings, some of the top-rated restaurants in Indiana include:\n",
      "\n",
      "* St. Elmo Steak House in Indianapolis: Known for its steaks and seafood, this restaurant has been a staple in Indiana for over 100 years.\n",
      "* The Eagle in Indianapolis: This farm-to-table restaurant offers a seasonal menu featuring locally sourced ingredients.\n",
      "* Bluebeard in Indianapolis: This upscale restaurant\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Call gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "# Generate text using fine-tuned model\n",
    "query = \"What is best restaurent in Indiana state US?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cade990a-19f9-496b-b31d-37a5a2b42e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Does Majestic Security offer transportation escort services?\n",
      "Context: {\"name\": \"Majestic Security\", \"address\": \"Majestic Security, 3128 Lexington Park Dr, Elkhart, IN 46514\", \"gmap_id\": \"0x8816c4b2fb8fb6a1:0x80451636e10ca83f\", \"description\": null, \"latitude\": 41.6899261, \"longitude\": -86.02416989999999, \"category\": [\"Security guard service\", \"Business to business service\", \"Public safety office\", \"Security service\", \"Training centre\", \"Training school\", \"Transportation escort service\"], \"avg_rating\": 4.3, \"num_of_reviews\": 48, \"price\": null, \"hours\": [[\"Thursday\", \"9AM–5PM\"], [\"Friday\", \"9AM–5PM\"], [\"Saturday\", \"Closed\"], [\"Sunday\", \"Closed\"], [\"Monday\", \"9AM–5PM\"], [\"Tuesday\", \"9AM–5PM\"], [\"Wednesday\", \"9AM–5PM\"]], \"MISC\": null, \"state\": \"Open ⋅ Closes 5PM\", \"relative_results\": [\"0x8816e8092cc37eff:0xa138075153591bc7\", \"0x8816ce61cc404e23:0x71a5e9e0898036a4\", \"0x8816e9eb8afbc539:0x7d7ee677df3fafa3\", \"0x8816cd46eed45c35:0x7d80db2d3b489fc3\", \"0x8816ebe5c65cf3ad:0xe8dede77091f4ecf\"], \"url\": \"https://www.google.com/maps/place//data=!4m2!3m1!1s0x8816c4b2fb8fb6a1:0x80451636e10ca83f?authuser=-1&hl=en&gl=us\"}\n",
      "\n",
      "<ANSWER>: Yes, Majestic Security offers transportation escort services. According to the business information, \"Transportation escort service\" is listed under the category of services provided by Majestic Security. ##begin_quote##Transportation escort service##end_quote##. This indicates that they do offer this type of service. <ANSWER>: Yes. ($\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the text to be processed\n",
    "data = {\n",
    "    'text': 'Question: Does Majestic Security offer transportation escort services?\\nContext: {\"name\": \"Majestic Security\", \"address\": \"Majestic Security, 3128 Lexington Park Dr, Elkhart, IN 46514\", \"gmap_id\": \"0x8816c4b2fb8fb6a1:0x80451636e10ca83f\", \"description\": null, \"latitude\": 41.6899261, \"longitude\": -86.02416989999999, \"category\": [\"Security guard service\", \"Business to business service\", \"Public safety office\", \"Security service\", \"Training centre\", \"Training school\", \"Transportation escort service\"], \"avg_rating\": 4.3, \"num_of_reviews\": 48, \"price\": null, \"hours\": [[\"Thursday\", \"9AM\\u20135PM\"], [\"Friday\", \"9AM\\u20135PM\"], [\"Saturday\", \"Closed\"], [\"Sunday\", \"Closed\"], [\"Monday\", \"9AM\\u20135PM\"], [\"Tuesday\", \"9AM\\u20135PM\"], [\"Wednesday\", \"9AM\\u20135PM\"]], \"MISC\": null, \"state\": \"Open \\u22c5 Closes 5PM\", \"relative_results\": [\"0x8816e8092cc37eff:0xa138075153591bc7\", \"0x8816ce61cc404e23:0x71a5e9e0898036a4\", \"0x8816e9eb8afbc539:0x7d7ee677df3fafa3\", \"0x8816cd46eed45c35:0x7d80db2d3b489fc3\", \"0x8816ebe5c65cf3ad:0xe8dede77091f4ecf\"], \"url\": \"https://www.google.com/maps/place//data=!4m2!3m1!1s0x8816c4b2fb8fb6a1:0x80451636e10ca83f?authuser=-1&hl=en&gl=us\"}\\n\\n<ANSWER>:',\n",
    "    'COTAnswer': 'According to the provided information, Majestic Security has an average rating of 4.3 out of 5 stars based on 48 customer reviews. While there are no specific quotes from previous customers mentioning the quality of security guards, we can infer that most customers have had a positive experience with the company.',\n",
    "    'Answer':'The majority of previous customers have been satisfied with the quality of security guards provided by Majestic Security.'\n",
    "}\n",
    "\n",
    "# Generate text using the fine-tuned model\n",
    "query = data['text']\n",
    "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "output = text_gen(f\"{query}\")\n",
    "\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b466-425a-4ab9-8667-fee456e86f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
