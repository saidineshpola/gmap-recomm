{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19afbda-01d4-4208-a2d9-4597c5c1b61b",
   "metadata": {},
   "source": [
    "# Step1 getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbd667-f65e-42d7-85b0-a293edc8dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rocm-smi --showproductname\n",
    "# sudo apt install libdrm-dev libsystemd-dev nvtop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555cfdef-04a6-4ebe-b7bd-24b4cf01830d",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f30fb-5d5e-4533-b3bf-992222f41ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!sudo pip install  peft transformers trl accelerate # results_modified_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac33e40-9de6-40ff-9bdb-00b73bbccaaa",
   "metadata": {},
   "source": [
    "## Install bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948a4e1-7c3e-48f0-9b57-d5ce35b98765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install `bitsandbytes`\n",
    "!git clone --recurse https://github.com/ROCm/bitsandbytes.git\n",
    "%cd /home/aac/bitsandbytes\n",
    "!git checkout rocm_enabled\n",
    "!make hip\n",
    "!python setup.py install\n",
    "%cd /home/aac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a7eac-36c0-4a37-9047-e964a7ed0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78258a-3043-43a4-baa6-7b8b256c026c",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bc6012-7f26-4fa8-993c-2ac94bd48b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e39cc-0c64-4b87-b231-b499f8ba1cbc",
   "metadata": {},
   "source": [
    "# Step 2: Configuring the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87662be1-1745-412e-bade-c7314a551de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN']='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'\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"Meta-Llama-3-8B-Instruct\" # \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model_name = \"llama-3-8b-gmap-recomm\" #You can give your own name for fine tuned model\n",
    "\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct', trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    \n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "# base_model.save_pretrained('Meta-Llama-3-8B-Instruct')\n",
    "# llama_tokenizer.save_pretrained('Meta-Llama-3-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e549edaa-90c9-420e-8db9-d6c68022a986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Data set\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "# check the data\n",
    "print(training_data.shape)\n",
    "# #11 is a QA sample in English\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaf5542-eda1-42d8-af04-38b4cd82617e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Combine relevant fields into a single text field\n",
    "    texts = [\n",
    "        f\"Question: {q}\\nContext: {c}\\nCOTAnswer: {a}\"\n",
    "        for q, c, a in zip(examples['question'], examples['oracle_context'], examples['cot_answer'])\n",
    "    ]\n",
    "    \n",
    "    # Return a dictionary with the new 'text' field\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def prepare_dataset_for_sft(dataset_path):\n",
    "    # Load the dataset\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    preprocessed_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names  # Remove original columns\n",
    "    )\n",
    "    \n",
    "    # Set the format to PyTorch tensors\n",
    "    preprocessed_dataset.set_format(type=\"torch\")\n",
    "    return preprocessed_dataset\n",
    "\n",
    "training_data = prepare_dataset_for_sft(\n",
    "    dataset_path=\"raft/temp_data\",\n",
    ")\n",
    "#training_data.save_to_disk('datasets/dataset_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b9d446-7793-4a0b-89e8-ec9054c57702",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_from_disk('datasets/dataset_preprocessed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85846b2a-ca19-4577-afa0-20607df56f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Question: Are there any reviews from previous customers that mention the quality of security guards provided by Majestic Security?\\nContext: {\"name\": \"Majestic Security\", \"address\": \"Majestic Security, 3128 Lexington Park Dr, Elkhart, IN 46514\", \"gmap_id\": \"0x8816c4b2fb8fb6a1:0x80451636e10ca83f\", \"description\": null, \"latitude\": 41.6899261, \"longitude\": -86.02416989999999, \"category\": [\"Security guard service\", \"Business to business service\", \"Public safety office\", \"Security service\", \"Training centre\", \"Training school\", \"Transportation escort service\"], \"avg_rating\": 4.3, \"num_of_reviews\": 48, \"price\": null, \"hours\": [[\"Thursday\", \"9AM\\\\u20135PM\"], [\"Friday\", \"9AM\\\\u20135PM\"], [\"Saturday\", \"Closed\"], [\"Sunday\", \"Closed\"], [\"Monday\", \"9AM\\\\u20135PM\"], [\"Tuesday\", \"9AM\\\\u20135PM\"], [\"Wednesday\", \"9AM\\\\u20135PM\"]], \"MISC\": null, \"state\": \"Open \\\\u22c5 Closes 5PM\", \"relative_results\": [\"0x8816e8092cc37eff:0xa138075153591bc7\", \"0x8816ce61cc404e23:0x71a5e9e0898036a4\", \"0x8816e9eb8afbc539:0x7d7ee677df3fafa3\", \"0x8816cd46eed45c35:0x7d80db2d3b489fc3\", \"0x8816ebe5c65cf3ad:0xe8dede77091f4ecf\"], \"url\": \"https://www.google.com/maps/place//data=!4m2!3m1!1s0x8816c4b2fb8fb6a1:0x80451636e10ca83f?authuser=-1&hl=en&gl=us\"}\\nAnswer: According to the provided information, Majestic Security has an average rating of 4.3 out of 5 stars based on 48 customer reviews. While there are no specific quotes from previous customers mentioning the quality of security guards, we can infer that most customers have had a positive experience with the company.\\n\\n<ANSWER>: The majority of previous customers have been satisfied with the quality of security guards provided by Majestic Security.'}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e60c79-ef64-472d-803e-f60c84678087",
   "metadata": {},
   "source": [
    "# Step 3: Start fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af771954-905a-43af-9ea4-960a58915663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified_v1\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #optim=\"paged_adamw_32bit\",\n",
    "    save_steps=4000,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4520f-04e3-4c2e-8c13-f3fd8d89dfea",
   "metadata": {},
   "source": [
    "Training with LoRA configuration\n",
    "Now you can integrate LoRA into the base model and assess its additional parameters. LoRA essentially adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains the newly added weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd52f9ff-6727-4657-9c3e-547d60c1e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/opt/conda/envs/py_3.9/lib/python3.9/site-packages/bitsandbytes-0.44.0.dev0-py3.9-linux-x86_64.egg/bitsandbytes/libbitsandbytes_hip.so')\n",
      "Could not load bitsandbytes native library: /opt/conda/envs/py_3.9/lib/python3.9/site-packages/bitsandbytes-0.44.0.dev0-py3.9-linux-x86_64.egg/bitsandbytes/libbitsandbytes_cpu.so: cannot open shared object file: No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/bitsandbytes-0.44.0.dev0-py3.9-linux-x86_64.egg/bitsandbytes/cextension.py\", line 124, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/bitsandbytes-0.44.0.dev0-py3.9-linux-x86_64.egg/bitsandbytes/cextension.py\", line 104, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/opt/conda/envs/py_3.9/lib/python3.9/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/opt/conda/envs/py_3.9/lib/python3.9/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /opt/conda/envs/py_3.9/lib/python3.9/site-packages/bitsandbytes-0.44.0.dev0-py3.9-linux-x86_64.egg/bitsandbytes/libbitsandbytes_cpu.so: cannot open shared object file: No such file or directory\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    target_modules= ['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4ad53-3ee7-4005-a158-e4e6f7722289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.9/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2070' max='32130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2070/32130 28:55 < 7:00:25, 1.19 it/s, Epoch 0.13/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.109400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer with LoRA configuration\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Training\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbbe50-518f-45b2-b8e8-41fc7074181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dc7d3-16e6-4787-b014-bb5427259860",
   "metadata": {},
   "source": [
    "# Step 4: Test the fine-tuned model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d970036-8fce-4b01-b0e8-88a06f27fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "del base_model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "model = model.merge_and_unload()\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30b19f-2893-4e90-b23a-9278039a42c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Call gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "# Generate text using fine-tuned model\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade990a-19f9-496b-b31d-37a5a2b42e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
